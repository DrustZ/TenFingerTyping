{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TenFingerDatasetOld(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, char2label, label2char,\n",
    "                 max_seq_length=256, ignore_label_id=-100, \\\n",
    "                 pad_token=0, debug=False):\n",
    "        self.samples = []\n",
    "        self.char2label = char2label\n",
    "        self.label2char = label2char\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.special_tokens_count = tokenizer.num_added_tokens()+1\n",
    "        self.ignore_label_id = ignore_label_id\n",
    "        self.pad_token = pad_token\n",
    "        self.debug = debug\n",
    "        \n",
    "        with open(data_path) as f:\n",
    "            for line in f.readlines():\n",
    "                self.samples.append(line.strip())\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.strToSample(self.samples[index])\n",
    "    \n",
    "    def char2finger(self, c):\n",
    "        c2f = {\n",
    "            'q':1, 'a':1, 'z':1,\n",
    "            'w':2, 's':2, 'x':2,\n",
    "            'e':3, 'd':3, 'c':3,\n",
    "            'r':4, 'f':4, 'v':4,\n",
    "            't':4, 'g':4, 'b':4,\n",
    "            'y':5, 'h':5, 'n':5,\n",
    "            'u':5, 'j':5, 'm':5,\n",
    "            'i':6, 'k':6,\n",
    "            'o':7, 'l':7,\n",
    "            'p':8,\n",
    "        }\n",
    "        if c == ' ':\n",
    "            return 1 #unused 0\n",
    "        if c in c2f:\n",
    "            return 1+c2f[c] #unused 1 - 8\n",
    "        return 10 #unused 9\n",
    "    \n",
    "    def strToSample(self, content):\n",
    "        tokens = content.split()\n",
    "        #we randomly select the start index of typing\n",
    "        #and give 0 more chance\n",
    "        typing_start = random.choice(\n",
    "            list(range(len(tokens)))+[0]*2)\n",
    "        #the pre context of a sample\n",
    "        pre_tokens = tokens[:typing_start]\n",
    "        pre_tokens = tokenizer.tokenize(' '.join(pre_tokens))\n",
    "\n",
    "        typing_text = ' '.join(tokens[typing_start:])\n",
    "        typing_seq = [self.char2finger(c) for c in typing_text]\n",
    "\n",
    "        #if typing seq is longer than max seq\n",
    "        if len(typing_seq) > self.max_seq_length - self.special_tokens_count:\n",
    "            typing_text = typing_text[:(self.max_seq_length - self.special_tokens_count)]\n",
    "            typing_text = ' '.join(typing_text.split()[:-1])\n",
    "            typing_seq = [self.char2finger(c) for c in typing_text]\n",
    "            pre_tokens = []\n",
    "\n",
    "        #else if typing+token is longer than max seq\n",
    "        extra = len(pre_tokens)+len(typing_seq)-\\\n",
    "                (self.max_seq_length - self.special_tokens_count)\n",
    "        if extra > 0:\n",
    "            pre_tokens = pre_tokens[extra:]\n",
    "\n",
    "        # The sample format:\n",
    "        # [precontext] what is your [typing] k e y\n",
    "        # [CLS] token_id token_id token_id [SEP] finger_id finger_id finger_id [SEP]\n",
    "\n",
    "        pre_ids = self.tokenizer.convert_tokens_to_ids(['[CLS]']+pre_tokens+['[SEP]'])\n",
    "        input_ids = pre_ids+typing_seq+self.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "\n",
    "        label_ids = len(pre_ids)*[self.ignore_label_id]+\\\n",
    "                [self.char2label(c) for c in typing_text]+\\\n",
    "                [self.ignore_label_id]\n",
    "\n",
    "        segment_ids = len(pre_ids)*[0]+(len(typing_text)+1)*[1]\n",
    "        input_mask = len(label_ids)*[1]\n",
    "\n",
    "        padding_len = self.max_seq_length - len(input_ids)\n",
    "        input_ids += [self.pad_token]*padding_len\n",
    "        input_mask += [self.pad_token]*padding_len\n",
    "        segment_ids += [self.pad_token]*padding_len\n",
    "        label_ids += [self.ignore_label_id]*padding_len\n",
    "\n",
    "        if self.debug:\n",
    "            print('typing text: %s' % typing_text)\n",
    "            print(\"tokens: \", \" \".join([str(x) for x in pre_tokens]))\n",
    "            print(\"pre_ids: \", \" \".join([str(x) for x in pre_ids]))\n",
    "            print(\"input_ids: \", \" \".join([str(x) for x in input_ids]))\n",
    "            print(\"input_mask: \", \" \".join([str(x) for x in input_mask]))\n",
    "            print(\"segment_ids: \", \" \".join([str(x) for x in segment_ids]))\n",
    "            print(\"label_ids: \", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        assert len(input_ids) == self.max_seq_length\n",
    "        assert len(input_mask) == self.max_seq_length\n",
    "        assert len(segment_ids) == self.max_seq_length\n",
    "        assert len(label_ids) == self.max_seq_length\n",
    "\n",
    "        return input_ids, input_mask, segment_ids, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    input_ids, input_mask, segment_ids, label_ids = zip(*batch)\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    input_mask = torch.LongTensor(input_mask)\n",
    "    segment_ids = torch.LongTensor(segment_ids)\n",
    "    label_ids = torch.LongTensor(label_ids)\n",
    "    return (input_ids, input_mask, segment_ids, label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load** the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "def char2label(ch):\n",
    "  c2l = {c: i for i, c in enumerate(string.ascii_lowercase+' ')}\n",
    "  if ch in c2l:\n",
    "    return c2l[ch]\n",
    "  else:\n",
    "    return len(c2l)\n",
    "\n",
    "def label2char(ii):\n",
    "  l2c = {i: c for i, c in enumerate(string.ascii_lowercase+' ')}\n",
    "  if ii in l2c:\n",
    "    return l2c[ii]\n",
    "  else:\n",
    "    return '*'\n",
    "\n",
    "train_dataset = TenFingerDatasetOld(\n",
    "    data_path=\"data/yelpamazon.txt\",\n",
    "    char2label=char2label, label2char=label2char, tokenizer=tokenizer)\n",
    "\n",
    "test_dataset = TenFingerDatasetOld(\n",
    "    data_path=\"data/testmovie.txt\",\n",
    "    char2label=char2label, label2char=label2char, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    BertForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "prepath = 'Models/local/checkpoint-730000/'\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "        prepath,\n",
    "        num_labels=len(string.ascii_lowercase+' ')+1,\n",
    "        cache_dir=None,\n",
    "    )\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    prepath,\n",
    "    config=config,)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(model, pad_token_label_id, num_workers=0, batch_size=24, prefix=\"\"):\n",
    "    test_data = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True,\n",
    "                       num_workers=num_workers, collate_fn=collate)\n",
    "    # Eval!\n",
    "    print(\"***** Running evaluation %s *****\", prefix)\n",
    "    print(\"  Num examples = %d\", len(test_data))\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1],\n",
    "                      \"token_type_ids\": batch[2], \"labels\": batch[3]}\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label2char(out_label_ids[i][j]))\n",
    "                preds_list[i].append(label2char(preds[i][j]))\n",
    "\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }\n",
    "\n",
    "    print(\"***** Eval results %s *****\", prefix)\n",
    "    for key in sorted(results.keys()):\n",
    "        print(\"  %s = %s\" % (key, str(results[key])))\n",
    "\n",
    "    return results, preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, log_dir, outdir_prefix='',\n",
    "          num_workers=0, batch_size=28, pad_token_label_id=-100,\n",
    "          gradient_accumulation_steps=1, \n",
    "          num_train_epochs=2.0, learning_rate=5e-5, model_name_or_path=\"\",\n",
    "          logging_steps=1000, testing_steps=10000, save_steps=10000):\n",
    "  \n",
    "  tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "  train_data = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, \n",
    "                        num_workers=num_workers, collate_fn=collate)\n",
    "\n",
    "  t_total = len(train_data) // gradient_accumulation_steps * num_train_epochs\n",
    "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "  optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "  scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=5, num_training_steps=t_total)\n",
    "  \n",
    "      # Check if saved optimizer or scheduler states exist\n",
    "  if os.path.isfile(os.path.join(model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "        os.path.join(model_name_or_path, \"scheduler.pt\")\n",
    "  ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "  print(\"***** Running training *****\")\n",
    "  print(\"  Num examples = %d\" % len(train_data))\n",
    "  print(\"  Num Epochs = %d\" % num_train_epochs)\n",
    "  print(\"  Total optimization steps = %d\" % t_total)\n",
    "\n",
    "  global_step = 0\n",
    "  epochs_trained = 0\n",
    "  steps_trained_in_current_epoch = 0\n",
    "\n",
    "  # Check if continuing training from a checkpoint\n",
    "  if os.path.exists(model_name_or_path):\n",
    "    # set global_step to gobal_step of last saved checkpoint from model path\n",
    "      try:\n",
    "        global_step = int(model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "      except ValueError:\n",
    "        global_step = 0\n",
    "      epochs_trained = global_step // (len(train_data) // gradient_accumulation_steps)\n",
    "      steps_trained_in_current_epoch = global_step % (len(train_data) // gradient_accumulation_steps)\n",
    "\n",
    "      print(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "      print(\"  Continuing training from epoch %d\" % epochs_trained)\n",
    "      print(\"  Continuing training from global step %d\" % global_step)\n",
    "      print(\"  Will skip the first %d steps in the first epoch\" % steps_trained_in_current_epoch)\n",
    "\n",
    "  tr_loss, logging_loss = 0.0, 0.0\n",
    "  model.zero_grad()\n",
    "  train_iterator = trange(\n",
    "        epochs_trained, int(num_train_epochs), desc=\"Epoch\")\n",
    "  \n",
    "  t0 = time.time()\n",
    "  for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_data, desc=\"Iteration\")\n",
    "\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \n",
    "                      \"token_type_ids\": batch[2], \"labels\": batch[3]}\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                if global_step % logging_steps == 0: \n",
    "                  tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "                  tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / logging_steps, global_step)\n",
    "                  logging_loss = tr_loss\n",
    "                \n",
    "                if global_step % testing_steps == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(\"eval step \", global_step)\n",
    "                    results, _ = evaluate(model, pad_token_label_id,  \n",
    "                                          num_workers=2,\n",
    "                                          batch_size=batch_size)\n",
    "                    for key, value in results.items():\n",
    "                      tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    \n",
    "\n",
    "                if global_step % save_steps == 0:\n",
    "                    # Save model checkpoint to gDrive\n",
    "                    output_dir = os.path.join(outdir_prefix, \"checkpoint-{}\".format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    print(\"Saving optimizer and scheduler states to %s\" % output_dir)\n",
    "\n",
    "        avg_train_loss = tr_loss / len(train_data)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "  tb_writer.close()\n",
    "  return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "logname = 'yelpamazon_lr0.3/'\n",
    "log_dir = 'logs/'+logname\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "global_step, tr_loss = train(model, log_dir=log_dir, num_workers=2,\n",
    "                             batch_size=12, learning_rate=3e-5,\n",
    "                             gradient_accumulation_steps=2,\n",
    "                             logging_steps=100, testing_steps=10000, save_steps=10000,\n",
    "                             model_name_or_path=prepath,\n",
    "                             outdir_prefix='Models/local')\n",
    "print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
