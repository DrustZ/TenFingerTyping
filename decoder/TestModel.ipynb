{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1080\n",
      "0\n",
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def char2finger(c):\n",
    "    c2f = {\n",
    "        'q':1, 'a':1, 'z':1,\n",
    "        'w':2, 's':2, 'x':2,\n",
    "        'e':3, 'd':3, 'c':3,\n",
    "        'r':4, 'f':4, 'v':4,\n",
    "        't':4, 'g':4, 'b':4,\n",
    "        'y':5, 'h':5, 'n':5,\n",
    "        'u':5, 'j':5, 'm':5,\n",
    "        'i':6, 'k':6,\n",
    "        'o':7, 'l':7,\n",
    "        'p':8,\n",
    "    }\n",
    "    if c == ' ':\n",
    "        return 1 #unused 0\n",
    "    if c in c2f:\n",
    "        return 1+c2f[c] #unused 1 - 8\n",
    "    return 10 #unused 9\n",
    "\n",
    "def char2label(ch):\n",
    "  c2l = {c: i for i, c in enumerate(string.ascii_lowercase+' ')}\n",
    "  if ch in c2l:\n",
    "    return c2l[ch]\n",
    "  else:\n",
    "    return len(c2l)\n",
    "\n",
    "def label2char(ii):\n",
    "  l2c = {i: c for i, c in enumerate(string.ascii_lowercase+' ')}\n",
    "  if ii in l2c:\n",
    "    return l2c[ii]\n",
    "  else:\n",
    "    return '*'\n",
    "\n",
    "special_tokens_count = tokenizer.num_added_tokens()+1\n",
    "max_seq_length = 256\n",
    "ignore_label_id = -100\n",
    "pad_token = 0\n",
    "\n",
    "def strToSample(content, debug = False):\n",
    "    tokens = content.split()\n",
    "    #we randomly select the start index of typing\n",
    "    #and give 0 more chance\n",
    "    typing_start = random.choice(\n",
    "        list(range(len(tokens)))+[0]*2)\n",
    "    typing_start = 0\n",
    "    #the pre context of a sample\n",
    "    pre_tokens = tokens[:typing_start]\n",
    "    pre_tokens = tokenizer.tokenize(' '.join(pre_tokens))\n",
    "\n",
    "    typing_text = ' '.join(tokens[typing_start:])\n",
    "    typing_seq = [char2finger(c) for c in typing_text]\n",
    "\n",
    "    #if typing seq is longer than max seq\n",
    "    if len(typing_seq) > max_seq_length - special_tokens_count:\n",
    "        typing_text = typing_text[:(max_seq_length - special_tokens_count)]\n",
    "        typing_text = ' '.join(typing_text.split()[:-1])\n",
    "        typing_seq = [char2finger(c) for c in typing_text]\n",
    "        pre_tokens = []\n",
    "\n",
    "    #else if typing+token is longer than max seq\n",
    "    extra = len(pre_tokens)+len(typing_seq)-\\\n",
    "            (max_seq_length - special_tokens_count)\n",
    "    if extra > 0:\n",
    "        pre_tokens = pre_tokens[extra:]\n",
    "\n",
    "    # The sample format:\n",
    "    # [precontext] what is your [typing] k e y\n",
    "    # [CLS] token_id token_id token_id [SEP] finger_id finger_id finger_id [SEP]\n",
    "\n",
    "    pre_ids = tokenizer.convert_tokens_to_ids(['[CLS]']+pre_tokens+['[SEP]'])\n",
    "    input_ids = pre_ids+typing_seq+tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "\n",
    "    label_ids = len(pre_ids)*[ignore_label_id]+\\\n",
    "            [char2label(c) for c in typing_text]+\\\n",
    "            [ignore_label_id]\n",
    "\n",
    "    segment_ids = len(pre_ids)*[0]+(len(typing_text)+1)*[1]\n",
    "    input_mask = len(label_ids)*[1]\n",
    "\n",
    "    padding_len = max_seq_length - len(input_ids)\n",
    "    input_ids += [pad_token]*padding_len\n",
    "    input_mask += [pad_token]*padding_len\n",
    "    segment_ids += [pad_token]*padding_len\n",
    "    label_ids += [ignore_label_id]*padding_len\n",
    "\n",
    "    if debug:\n",
    "        print('typing text: %s' % typing_text)\n",
    "        print(\"tokens: \", \" \".join([str(x) for x in pre_tokens]))\n",
    "        print(\"pre_ids: \", \" \".join([str(x) for x in pre_ids]))\n",
    "        print(\"input_ids: \", \" \".join([str(x) for x in input_ids]))\n",
    "        print(\"input_mask: \", \" \".join([str(x) for x in input_mask]))\n",
    "        print(\"segment_ids: \", \" \".join([str(x) for x in segment_ids]))\n",
    "        print(\"label_ids: \", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_path = 'Models/local/Bestcheckpoint-1370000/'\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    BertForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "        pretrained_path,\n",
    "        num_labels=len(string.ascii_lowercase+' ')+1,\n",
    "        cache_dir=None,\n",
    "    )\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    pretrained_path,\n",
    "    config=config,)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testcase = strToSample(\"elephants are afr\")\n",
    "testcase = [torch.LongTensor(t) for t in testcase]\n",
    "testcase = [t.unsqueeze(0) for t in testcase]\n",
    "testcase = tuple(t.to(device) for t in testcase)\n",
    "inputs = {\"input_ids\": testcase[0], \"attention_mask\": testcase[1],\n",
    "                      \"token_type_ids\": testcase[2], \"labels\": testcase[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = np.array(testcases).transpose((1,0,2))\n",
    "tn = torch.LongTensor(tn)\n",
    "tn = tn.to(device)\n",
    "inputs = {\"input_ids\": tn[0], \"attention_mask\": tn[1],\n",
    "                      \"token_type_ids\": tn[2], \"labels\": tn[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search for transposition/omission error correction\n",
    "import torch.nn.functional as F\n",
    "\n",
    "fingerlist = 'asdfjklp'\n",
    "\n",
    "def searchInsertionNTranspositions(searchlists):\n",
    "    testcases = np.array([strToSample(seq) for seq in searchlists])\n",
    "    testcases = testcases.transpose((1,0,2)) # (batch, item, input) to (item, batch, input)\n",
    "    testcases = torch.LongTensor(testcases).to(device)\n",
    "    inputs = {\"input_ids\": testcases[0], \"attention_mask\": testcases[1],\n",
    "                              \"token_type_ids\": testcases[2], \"labels\": testcases[3]}\n",
    "    outputs = model(**inputs)\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "    probs = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "    return probs, out_label_ids\n",
    "\n",
    "def searchAlternatives(inputseqs):\n",
    "    problists = []\n",
    "    labellists = []\n",
    "    \n",
    "    searchlists = []\n",
    "    for seq in inputseqs:\n",
    "        searchlists += [seq]\n",
    "        if len(seq) == len(inputseqs[0]):\n",
    "            searchlists += [seq[:-1]+c+seq[-1] for c in fingerlist]\n",
    "        if len(seq) > 1 and ' ' not in seq[-3:]:\n",
    "            searchlists.append(seq[:-2]+seq[-1]+seq[-2])\n",
    "        searchlists = list(dict.fromkeys(searchlists))\n",
    "    for i in range(0, len(searchlists), 20):\n",
    "        probs, out_label_ids = searchInsertionNTranspositions(searchlists[i:i+20])\n",
    "        if len(problists) == 0:\n",
    "            problists = probs\n",
    "            labellists = out_label_ids\n",
    "        else:\n",
    "            problists = np.concatenate((problists, probs), axis=0)\n",
    "            labellists = np.concatenate((labellists, out_label_ids), axis=0)\n",
    "    probsmax = np.max(problists, axis=-1)\n",
    "    preds = np.argmax(problists, axis=-1)\n",
    "    valid_probs = probsmax * (labellists != ignore_label_id)\n",
    "    avg_probs = []\n",
    "    for i in range(len(valid_probs)):\n",
    "        probsum = valid_probs[i].sum()\n",
    "        avg_probs.append(probsum / (valid_probs[i] > 0).sum())\n",
    "    avg_probs = np.array(avg_probs)\n",
    "    avg_probs[1:] -= 0.03\n",
    "    top_indices = (-avg_probs).argsort()[:10]\n",
    "    #always the original sequence (the 0-index)\n",
    "    top_indices = [0] + list(top_indices[top_indices!=0])\n",
    "    print(valid_probs.shape, avg_probs.shape, searchlists[0], top_indices, (valid_probs[0] > 0).sum())\n",
    "    return preds[top_indices], labellists[top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, label_ids = searchAlternatives(['informaion', 'informalialn', 'informailn', 'informacialn', 'information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['informaion',\n",
       " 'information',\n",
       " 'informaion',\n",
       " 'informacialy',\n",
       " 'informailly',\n",
       " 'informailly',\n",
       " 'informailty',\n",
       " 'informailty',\n",
       " 'informaliano',\n",
       " 'informailin']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(len(label_ids)):\n",
    "    resid = preds[i][ label_ids[i] != ignore_label_id ]\n",
    "    res.append(''.join([label2char(c) for c in resid]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 256) (9,) i [0, 7, 1, 8, 6, 2, 5, 3, 4] 1\n",
      "step 0: i res ['i', 'ok', 'ai', 'pi', 'ii']\n",
      "(13, 256) (13,) i e [0, 4, 1, 2, 6, 11, 8, 7, 10, 5] 3\n",
      "step 2: i e res ['i d', 'i ve', 'i ad', 'i we', 'i id']\n",
      "(26, 256) (26,) i en [0, 1, 10, 16, 7, 15, 6, 4, 13, 19, 22] 4\n",
      "step 3: i en res ['i en', 'i can', 'i can', 'i don', 'i don']\n",
      "(14, 256) (14,) i enj [0, 6, 4, 10, 11, 1, 8, 9, 3, 2] 5\n",
      "step 4: i enj res ['i dun', 'i chin', 'i envy', 'i damn', 'i damn']\n",
      "(26, 256) (26,) i enjo [0, 24, 5, 15, 21, 4, 14, 22, 20, 25, 8] 6\n",
      "step 5: i enjo res ['i duno', 'i canno', 'i dunno', 'i dunno', 'i emily']\n",
      "(25, 256) (25,) i enjoy [0, 10, 16, 6, 19, 9, 5, 15, 20, 22] 7\n",
      "step 6: i enjoy res ['i enjoy', 'i enjoy', 'i enjoin', 'i enjoin', 'i dunno']\n",
      "(19, 256) (19,) i enjoy p [0, 8, 1, 11, 7, 15, 18, 10, 9, 5] 9\n",
      "step 8: i enjoy p res ['i enjoy p', 'i enjoy pp', 'i enjoy ap', 'i dunno ap', 'i enjoy lp']\n",
      "(16, 256) (16,) i enjoy pl [0, 9, 4, 5, 8, 11, 1, 13, 7, 15] 10\n",
      "step 9: i enjoy pl res ['i enjoy po', 'i enjoy pop', 'i enjoy pro', 'i enjoy pho', 'i enjoy ppl']\n",
      "(27, 256) (27,) i enjoy pla [0, 20, 26, 18, 8, 7, 17, 2, 12, 24, 21] 11\n",
      "step 10: i enjoy pla res ['i enjoy pla', 'i enjoy plap', 'i enjoy ppal', 'i enjoy popa', 'i enjoy popa']\n",
      "(15, 256) (15,) i enjoy play [0, 6, 8, 11, 10, 5, 3, 2, 4, 1] 12\n",
      "step 11: i enjoy play res ['i enjoy play', 'i enjoy plain', 'i enjoy plapy', 'i enjoy ppalm', 'i enjoy plamp']\n",
      "(17, 256) (17,) i enjoy playn [0, 10, 9, 16, 6, 4, 15, 8, 11, 12] 13\n",
      "step 12: i enjoy playn res ['i enjoy plany', 'i enjoy plainy', 'i enjoy plany', 'i enjoy playup', 'i enjoy playin']\n",
      "(26, 256) (26,) i enjoy playng [0, 24, 3, 13, 21, 1, 11, 19, 9, 25, 20] 14\n",
      "step 13: i enjoy playng res ['i enjoy playug', 'i enjoy playing', 'i enjoy planner', 'i enjoy planner', 'i enjoy plainty']\n",
      "(21, 256) (21,) i enjoy playng i [0, 19, 18, 20, 7, 16, 10, 1, 9, 6] 16\n",
      "step 15: i enjoy playng i res ['i enjoy playug i', 'i enjoy planner i', 'i enjoy playing i', 'i enjoy plainty i', 'i enjoy playug ok']\n",
      "(23, 256) (23,) i enjoy playng in [0, 19, 20, 18, 15, 6, 9, 3, 12, 11] 17\n",
      "step 16: i enjoy playng in res ['i enjoy playng in', 'i enjoy playing in', 'i enjoy plainty in', 'i enjoy planner in', 'i enjoy playug kim']\n",
      "(18, 256) (18,) i enjoy playng inf [0, 5, 16, 13, 11, 4, 7, 6, 3, 15, 12] 18\n",
      "step 17: i enjoy playng inf res ['i enjoy playug ing', 'i enjoy playng kung', 'i enjoy playug king', 'i enjoy plainty ivy', 'i enjoy playing ibm']\n",
      "(28, 256) (28,) i enjoy playng info [0, 25, 27, 10, 4, 14, 6, 16, 19, 9] 19\n",
      "step 18: i enjoy playng info res ['i enjoy playng into', 'i enjoy plainty iron', 'i enjoy playing iron', 'i enjoy playng into', 'i enjoy playng intro']\n",
      "(26, 256) (26,) i enjoy playng infom [0, 23, 22, 21, 20, 4, 14, 5, 15, 7, 17] 20\n",
      "step 19: i enjoy playng infom res ['i enjoy playug infon', 'i enjoy playing irony', 'i enjoy playing irony', 'i enjoy plainty irony', 'i enjoy plainty irony']\n",
      "(24, 256) (24,) i enjoy playng infoma [0, 22, 21, 20, 23, 16, 6, 4, 14, 18] 21\n",
      "step 20: i enjoy playng infoma res ['i enjoy playug knfoma', 'i enjoy plainty ironya', 'i enjoy playing kroman', 'i enjoy playing ironya', 'i enjoy plainty iroman']\n",
      "(28, 256) (28,) i enjoy playng infomat [0, 26, 22, 23, 20, 24, 27, 21, 25, 5] 22\n",
      "step 21: i enjoy playng infomat res ['i enjoy playur kutomar', 'i enjoy plainty ironant', 'i enjoy playing ironant', 'i enjoy playing kromary', 'i enjoy plainty ironmat']\n",
      "(28, 256) (28,) i enjoy playng infomati [0, 27, 20, 22, 26, 25, 24, 16, 6, 15] 23\n",
      "step 22: i enjoy playng infomati res ['i enjoy playur kuromati', 'i enjoy plainty ironhair', 'i enjoy plainty iromanti', 'i enjoy playing iromanti', 'i enjoy plainty ironmark']\n",
      "(28, 256) (28,) i enjoy playng infomatio [0, 21, 26, 24, 20, 22, 10, 25, 23, 27] 24\n",
      "step 23: i enjoy playng infomatio res ['i enjoy playug intomario', 'i enjoy plainty kronnakov', 'i enjoy plainty ironnario', 'i enjoy playing kromanrio', 'i enjoy plainty kronyairo']\n",
      "(28, 256) (28,) i enjoy playng infomation [0, 22, 25, 23, 20, 24, 21, 4, 14, 27] 25\n",
      "step 24: i enjoy playng infomation res ['i enjoy playug intonation', 'i enjoy plainty ironnation', 'i enjoy playing iromantino', 'i enjoy plainty ironmarino', 'i enjoy plainty ironuailty']\n",
      "(22, 256) (22,) i enjoy playng infomation g [0, 18, 19, 20, 16, 7, 10, 1, 21, 15] 27\n",
      "step 26: i enjoy playng infomation g res ['i enjoy playug intonation b', 'i enjoy plainty ironnation b', 'i enjoy playing kromantino b', 'i enjoy plainty ironmarino b', 'i enjoy playug intonation of']\n",
      "(23, 256) (23,) i enjoy playng infomation ga [0, 19, 18, 20, 17, 8, 6, 15, 4, 13] 28\n",
      "step 27: i enjoy playng infomation ga res ['i enjoy playug intonation ta', 'i enjoy playing kromantino va', 'i enjoy plainty ironnation ta', 'i enjoy plainty ironmarino va', 'i enjoy playug intomation gpa']\n",
      "(28, 256) (28,) i enjoy playng infomation gam [0, 23, 22, 24, 20, 21, 25, 7, 17, 13] 29\n",
      "step 28: i enjoy playng infomation gam res ['i enjoy playug intonation fan', 'i enjoy plainty ironnation tha', 'i enjoy plainty ironnation fan', 'i enjoy plainty ironmarino fan', 'i enjoy playing kromantino fan']\n",
      "(28, 256) (28,) i enjoy playng infomation game [0, 26, 22, 23, 1, 11, 27, 10, 24, 25] 30\n",
      "step 29: i enjoy playng infomation game res ['i enjoy playug intonation game', 'i enjoy playing iromantino game', 'i enjoy plainty ironnation game', 'i enjoy plainty ironnation bach', 'i enjoy playug intonation gauze']\n",
      "(28, 256) (28,) i enjoy playng infomation games [0, 22, 20, 10, 14, 4, 23, 26, 24, 21] 31\n",
      "step 30: i enjoy playng infomation games res ['i enjoy playug intomation games', 'i enjoy plainty ironnation games', 'i enjoy playing iromantino games', 'i enjoy playug intomation games', 'i enjoy playug intonation gamers']\n",
      "(21, 256) (21,) i enjoy playng infomation games i [0, 18, 19, 20, 6, 15, 7, 16, 9, 10] 33\n",
      "step 32: i enjoy playng infomation games i res ['i enjoy playug intomation games i', 'i enjoy plainty ironnation games i', 'i enjoy playing iromantino games i', 'i enjoy playug intonation gamers i', 'i enjoy playug intomation games ii']\n",
      "(23, 256) (23,) i enjoy playng infomation games in [0, 18, 19, 20, 9, 15, 21, 6, 11, 2] 34\n",
      "step 33: i enjoy playng infomation games in res ['i enjoy playug intonation games in', 'i enjoy plainty ironnation games in', 'i enjoy playing iromantino games in', 'i enjoy playug intonation gamers in', 'i enjoy playug intonation games in']\n",
      "(21, 256) (21,) i enjoy playng infomation games in m [0, 18, 19, 10, 1, 5, 14, 7, 16, 6] 36\n",
      "step 35: i enjoy playng infomation games in m res ['i enjoy playug intonation games in n', 'i enjoy plainty ironnation games in n', 'i enjoy playing iromantino games in n', 'i enjoy playug intonation games in an', 'i enjoy playug intonation games in an']\n",
      "(22, 256) (22,) i enjoy playng infomation games in my [0, 18, 19, 20, 21, 6, 15, 9, 7, 16] 37\n",
      "step 36: i enjoy playng infomation games in my res ['i enjoy playug intonation games in my', 'i enjoy plainty ironnation games in my', 'i enjoy playing iromantino games in my', 'i enjoy playug intonation games in any', 'i enjoy playug intonation games in any']\n",
      "(21, 256) (21,) i enjoy playng infomation games in my s [0, 17, 8, 16, 7, 14, 5, 18, 10, 1] 39\n",
      "step 38: i enjoy playng infomation games in my s res ['i enjoy playug intonation games in my s', 'i enjoy playug infonation games in my ps', 'i enjoy playug infonation games in my ps', 'i enjoy playug infonation games in my os', 'i enjoy playug infonation games in my os']\n",
      "(22, 256) (22,) i enjoy playng infomation games in my sa [0, 21, 18, 19, 3, 12, 8, 17, 20, 5] 40\n",
      "step 39: i enjoy playng infomation games in my sa res ['i enjoy playug infonation games in my sa', 'i enjoy playug intonation games in my law', 'i enjoy playug infonation games in my psa', 'i enjoy playug infonation games in my paw', 'i enjoy playug intonation games in my sea']\n",
      "(28, 256) (28,) i enjoy playng infomation games in my sar [0, 24, 26, 6, 16, 20, 2, 12, 5, 15] 41\n",
      "step 40: i enjoy playng infomation games in my sar res ['i enjoy playug intonation games in my war', 'i enjoy playug intonation games in my past', 'i enjoy playug intonation games in my seat', 'i enjoy playug intonation games in my wait', 'i enjoy playug intonation games in my wait']\n",
      "(26, 256) (26,) i enjoy playng infomation games in my sare [0, 20, 24, 13, 3, 25, 21, 22, 7, 17] 42\n",
      "step 41: i enjoy playng infomation games in my sare res ['i enjoy playug intonation games in my save', 'i enjoy playug infonation games in my paste', 'i enjoy playug intonation games in my waite', 'i enjoy playug intonation games in my saved', 'i enjoy playug intonation games in my saved']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 256) (21,) i enjoy playng infomation games in my sare t [0, 18, 10, 1, 19, 7, 16, 20, 6, 15] 44\n",
      "step 43: i enjoy playng infomation games in my sare t res ['i enjoy playug intonation games in my save b', 'i enjoy playug infonation games in my paste b', 'i enjoy playug intonation games in my safe at', 'i enjoy playug intonation games in my safe at', 'i enjoy playug intonation games in my waite b']\n",
      "(22, 256) (22,) i enjoy playng infomation games in my sare ti [0, 20, 18, 21, 8, 17, 19, 5, 14, 6] 45\n",
      "step 44: i enjoy playng infomation games in my sare ti res ['i enjoy playug infonation games in my save ti', 'i enjoy playug intonation games in my safe air', 'i enjoy playug infonation games in my paste ti', 'i enjoy playug infonation games in my waite ti', 'i enjoy playug infonation games in my safe gpi']\n",
      "(28, 256) (28,) i enjoy playng infomation games in my sare tie [0, 16, 6, 17, 7, 13, 3, 5, 15, 8] 46\n",
      "step 45: i enjoy playng infomation games in my sare tie res ['i enjoy playug infonation games in my safe tid', 'i enjoy playug infonation games in my safe bike', 'i enjoy playug infonation games in my safe bike', 'i enjoy playug infonation games in my save file', 'i enjoy playug infonation games in my save file']\n",
      "(24, 256) (24,) i enjoy playng infomation games in my sare tiem [0, 4, 14, 19, 9, 10, 16, 6, 20, 3] 47\n",
      "step 46: i enjoy playng infomation games in my sare tiem res ['i enjoy playug intonation games in my safe rich', 'i enjoy playug intonation games in my safe fiery', 'i enjoy playug intonation games in my safe fiery', 'i enjoy playug intonation games in my safe time', 'i enjoy playug intonation games in my safe time']\n"
     ]
    }
   ],
   "source": [
    "typing = 'i enjoy playng infomation games in my sare tiem'\n",
    "alter = ['']\n",
    "for i in range(len(typing)):        \n",
    "    alter = [typing[:i]]+alter\n",
    "    alter = [phr+typing[i] for phr in alter]\n",
    "    alter = list(dict.fromkeys(alter))\n",
    "    if typing[i] == ' ':\n",
    "        continue\n",
    "    preds, label_ids = searchAlternatives(alter)\n",
    "    alter = []\n",
    "    for j in range(len(label_ids)):\n",
    "        resid = preds[j][ label_ids[j] != ignore_label_id ]\n",
    "        decoded = ''.join([label2char(c) for c in resid])\n",
    "        if len(decoded) <= i+2:\n",
    "            alter.append(decoded)\n",
    "            if len(alter) >= 5:\n",
    "                break\n",
    "    print(f'step {i}: {typing[:i+1]} res {alter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['are', 106744], ['age', 5186], ['abc', 685], ['ate', 408], ['arc', 352], ['ave', 75], ['abe', 60], ['atc', 58], ['ard', 55], ['qtc', 45], ['afc', 24], ['agc', 21], ['abd', 20]]\n",
      "are\n",
      "ate\n",
      "age\n",
      "ard\n",
      "arc\n",
      "abe\n",
      "afe\n",
      "ave\n",
      "ane\n",
      "ale\n",
      "aee\n",
      "qre\n",
      "ame\n",
      "aoe\n",
      "aue\n",
      "awe\n",
      "ahe\n",
      "ade\n",
      "ace\n",
      "axe\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Reading data back\n",
    "with open('dict.json', 'r') as f:\n",
    "    seqdict = json.load(f)\n",
    "    \n",
    "def beam_search(data, k):\n",
    "    sequences = [[[], 0]] #sequence, prob\n",
    "    for item in data:\n",
    "        sump = np.sum(np.exp(item))\n",
    "        eprob = np.log([np.exp(p)/sump for p in item])\n",
    "        new_seq = []\n",
    "        for sequence in sequences:\n",
    "            seq, prob = sequence\n",
    "            new_seq += [[seq+[idx], prob-p] for idx, p in enumerate(eprob)]\n",
    "        new_seq = sorted(new_seq, key=lambda tup:tup[1])\n",
    "        sequences = new_seq[:k]\n",
    "    return sequences\n",
    "H\n",
    "preds = logits.detach().cpu().numpy().squeeze()\n",
    "st, ed = words[1][0], words[1][1]\n",
    "seqs = beam_search(preds[st:ed], 20)\n",
    "tapseq = ''.join([str(n) for n in input_ids[st:ed]])\n",
    "print(seqdict[tapseq])\n",
    "\n",
    "for s in seqs:\n",
    "    print (''.join([label2char(idx) for idx in s[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 28])\n",
      "(256, 28)\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():     \n",
    "  outputs = model(**inputs)\n",
    "  print(outputs[1].shape)\n",
    "  tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "  preds = logits.detach().cpu().numpy().squeeze()[1]\n",
    "  out_label_ids = inputs[\"labels\"].detach().cpu().numpy().squeeze()\n",
    "  segment_ids = inputs[\"token_type_ids\"].detach().cpu().numpy().squeeze()\n",
    "  input_ids = inputs[\"input_ids\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "# words = []\n",
    "# wordstart = -1\n",
    "# for i in range(segment_ids.shape[0]):\n",
    "#     if segment_ids[i] != 0:\n",
    "#         if input_ids[i] in [1, 102]:\n",
    "#             words.append([wordstart, i])\n",
    "#             wordstart = -1\n",
    "#         elif wordstart == -1:\n",
    "#             wordstart = i\n",
    "\n",
    "# #beam search for each word\n",
    "# beam_size = 50\n",
    "# print(preds[words[0][0]])\n",
    "\n",
    "preds = np.argmax(preds, axis=1)\n",
    "preds_list = []\n",
    "for j in range(out_label_ids.shape[0]):\n",
    "  if out_label_ids[j] != ignore_label_id:\n",
    "    preds_list.append(label2char(preds[j]))\n",
    "\n",
    "print(''.join(preds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcases = []\n",
    "wrong = []\n",
    "\n",
    "with open('phrases2.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        testcases.append(line.strip().lower())\n",
    "\n",
    "for case in testcases:\n",
    "    testcase = strToSample(case)\n",
    "    testcase = [torch.LongTensor(t) for t in testcase]\n",
    "    testcase = [t.unsqueeze(0) for t in testcase]\n",
    "    testcase = tuple(t.to(device) for t in testcase)\n",
    "    inputs = {\"input_ids\": testcase[0], \"attention_mask\": testcase[1],\n",
    "                          \"token_type_ids\": testcase[2], \"labels\": testcase[3]}\n",
    "    with torch.no_grad():     \n",
    "      outputs = model(**inputs)\n",
    "      tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "      preds = logits.detach().cpu().numpy().squeeze()\n",
    "      out_label_ids = inputs[\"labels\"].detach().cpu().numpy().squeeze()\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    preds_list = []\n",
    "    for j in range(out_label_ids.shape[0]):\n",
    "      if out_label_ids[j] != ignore_label_id:\n",
    "        preds_list.append(label2char(preds[j]))\n",
    "\n",
    "    res = ''.join(preds_list)\n",
    "    if res != ' '.join(case.split()):\n",
    "        wrong.append([case, res])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i can see the rings on saturn', 'i can see the rings on satutn'], ['elephants are afraid of mice', 'elephants are afraid of nice'], ['if at first you do not succeed', 'it at first you do not succeed'], ['please provide your date of birth', 'please provide your care or birth'], ['we run the risk of failure', 'we buy the risk of failure'], ['beware the ides of march', 'beware the ices or march'], ['double double toil and trouble', 'double double foil and trouble'], ['play it again sam', 'play it again say'], ['you are not a jedi yet', 'you are not a heck yet'], ['starlight and dewdrop', 'starlight and desdtop'], ['drove my chevy to the levee', 'drove my ehevy to the legee'], ['but the levee was dry', 'but the leved was dry'], ['the quick brown fox jumped', 'the quick brown box jumped'], ['there will be some fog tonight', 'there will be some for tonight'], ['the dow jones index has risen', 'the cow jones index has risen'], ['we are subjects and must obey', 'we are subjects and just obey'], ['mom made her a turtleneck', 'you made her a turtleneck'], ['round robin scheduling', 'found robin scheduling'], ['all good boys deserve fudge', 'all good boys deserve there'], ['our fax number has changed', 'our gas number has changed'], ['hands on experience with a job', 'hands on experience with a hot'], ['a fox is a very smart animal', 'a box is a very smart animal'], ['a duck quacks to ask for food', 'a duck quacks to ask for good'], ['limited warranty of two years', 'limited warranty or two years'], ['dashing through the snow', 'dawhing through the show'], ['my bare face in the wind', 'my fave face in the wind'], ['i hate baking pies', 'i hate taking pics'], ['lydia wants to go home', 'lycia wants to go home'], ['win first prize in the contest', 'win first prize in the context'], ['freud wrote of the ego', 'trend store of the ego'], ['can we play cards tonight', 'day we play datds tonight'], ['he is shouting loudly', 'he is shouting londly'], ['the dog will bite you', 'the dog will give you'], ['where did you get that tie', 'where did you get that tid'], ['the largest of the five oceans', 'the largest of the fire oceans'], ['flashing red light means stop', 'flashing red light jeans stop'], ['my car always breaks in the winter', 'my cat always breaks in the winter'], ['the pen is mightier than the sword', 'the pen is nightier than the sword'], ['every apple from every tree', 'every apple from every free'], ['a yard is almost as long as a meter', 'a yard is almost as long as a never'], ['acutely aware of her good looks', 'acutely aware of her food looks'], ['obligations must be met first', 'obligations must be her first'], ['it looks like a shack', 'it looks like a snack'], ['the dog buried the bone', 'the dog buried the blue'], ['the generation gap gets wider', 'the generation rap gets wider'], ['sign the withdrawal slip', 'with the withdrawal slip'], ['the voters turfed him out', 'the roters rubbed him out'], ['never mix religion and politics', 'never his religion and politics'], ['players must know all the rules', 'players just know all the rules'], ['vote according to your conscience', 'gore according to your conscience'], ['sad to hear that news', 'sad to hear that mess'], ['the gun discharged by accident', 'the guy discharged by accident'], ['the fire blazed all weekend', 'the fire glazed all weekend'], ['the cat has a pleasant temperament', 'the car has a pleasant temperament'], ['he underwent triple bypass surgery', 'me underwent triple bypass surgery'], ['just like it says on the can', 'just like it says on the day'], ['companies announce a merger', 'companies announce a nerger'], ['salesmen must make their monthly quota', 'salesmen just make their monthly quota'], ['careless driving results in a fine', 'careless driving results in a time'], ['fine but only in moderation', 'time but only in moderation'], ['the fire raged for an entire month', 'the fire rated for an entire month'], ['an inefficient way to heat a house', 'an inefficient way to hear a house'], ['prescription drugs require a note', 'prescription drugs require a more'], ['the cream rises to the top', 'the dream rises to the top'], ['a tumor is ok provided it is benign', 'a rumor is ok provided it is benign'], ['knee bone is connected to the thigh bone', 'knee blue is connected to the thigh blue'], ['safe to walk the streets in the evening', 'save to walk the streets in the evening'], ['what goes up must come down', 'what goes up just come down'], ['learn to walk before you run', 'learn to walk before you buy'], ['tell a lie and your nose will grow', 'fell a lid and your nose will grow'], ['an enlarged nose suggests you are a liar', 'an enlarged hose suggests you are a liar'], ['lie detector tests never work', 'lid detector tests never work'], ['do not lie in court or else', 'do not lie in doubt or else'], ['only an idiot would lie in court', 'only an idiot would lie in doubt'], ['important news always seems to be late', 'important jews always seems to be late'], ['dormitory doors are locked at midnight', 'cormitory doors are locked at midnight'], ['questioning the wisdom of the courts', 'questioning the wisdom of the coubts'], ['that referendum asked a silly question', 'that referenchy asked a silly question'], ['put garbage in an abandoned mine', 'put garbage in an abandoned mind']]\n"
     ]
    }
   ],
   "source": [
    "print(wrong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
